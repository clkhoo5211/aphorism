#!/usr/bin/env python3
"""
Automated Divination Data Collection Script
Collects all 413 remaining lots from online sources

This script will:
1. Collect ËßÇÈü≥ÁÅµÁ≠æ 97 lots (4-100)
2. Collect ÈªÑÂ§ß‰ªô 99 lots (2-100)
3. Collect ÊúàËÄÅ 59 lots (2-60)
4. Collect ÂêïÁ•ñ 99 lots (2-100)
5. Collect Â¶àÁ•ñ 59 lots (2-60)

Total: 413 lots

Run this script:
    python3 collect_all_divination_data.py
"""

import json
import time
import random
import re
import sys
from typing import List, Dict, Optional
from pathlib import Path

try:
    import requests
    from bs4 import BeautifulSoup
    from requests.adapters import HTTPAdapter
    from urllib3.util.retry import Retry
except ImportError:
    print("‚ùå Missing dependencies. Install with: pip install -r requirements.txt")
    exit(1)

# Configuration
BASE_DELAY = 0.5  # Base delay between requests (seconds) - reduced for faster execution
MAX_RETRIES = 2  # Reduced retries for faster failure
TIMEOUT = 5  # Reduced timeout for faster failure detection
USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'

# Create session with retry strategy
def create_session():
    """Create a requests session with retry strategy"""
    session = requests.Session()
    retry_strategy = Retry(
        total=MAX_RETRIES,
        backoff_factor=1,
        status_forcelist=[429, 500, 502, 503, 504],
    )
    adapter = HTTPAdapter(max_retries=retry_strategy)
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    session.headers.update({'User-Agent': USER_AGENT})
    return session

session = create_session()

def safe_request(url: str, retries: int = 3) -> Optional[BeautifulSoup]:
    """Safely fetch and parse a webpage"""
    for attempt in range(retries):
        try:
            response = session.get(url, timeout=TIMEOUT)
            response.raise_for_status()
            response.encoding = 'utf-8'  # Try UTF-8 first
            if response.encoding.lower() == 'iso-8859-1':
                # Try to detect encoding
                try:
                    import chardet
                    detected = chardet.detect(response.content)
                    if detected and detected.get('encoding'):
                        response.encoding = detected['encoding']
                except ImportError:
                    # chardet not available, try common encodings
                    for encoding in ['gb2312', 'gbk', 'big5', 'utf-8']:
                        try:
                            response.content.decode(encoding)
                            response.encoding = encoding
                            break
                        except:
                            continue
            return BeautifulSoup(response.text, 'lxml')
        except requests.exceptions.HTTPError as e:
            # 404 and other client errors are permanent - don't retry
            if e.response and e.response.status_code in [404, 403, 401]:
                return None
            # Server errors might be transient - retry
            if attempt < retries - 1:
                time.sleep(BASE_DELAY * (2 ** attempt))
            else:
                return None
        except requests.exceptions.RequestException as e:
            # Network errors - retry
            if attempt < retries - 1:
                time.sleep(BASE_DELAY * (2 ** attempt))
            else:
                return None
    return None

def extract_text(element) -> str:
    """Safely extract text from BeautifulSoup element"""
    if element is None:
        return ""
    text = element.get_text(strip=True)
    return text.replace('\n', ' ').replace('\r', '').strip()

def normalize_fortune(text: str) -> str:
    """Normalize fortune text to standard format"""
    if not text:
        return "‰∏≠Á≠æ"
    
    text = text.strip()
    # Common fortune patterns
    if any(x in text for x in ["‰∏ä‰∏ä", "Â§ßÂêâ", "‰∏äÂêâ"]):
        return "‰∏ä‰∏äÁ≠æ"
    elif any(x in text for x in ["‰∏ä", "Âêâ", "‰∏≠‰∏ä"]):
        return "‰∏äÁ≠æ"
    elif any(x in text for x in ["‰∏≠", "Âπ≥"]):
        return "‰∏≠Á≠æ"
    elif any(x in text for x in ["‰∏≠‰∏ã", "‰∏ã"]):
        return "‰∏≠‰∏ãÁ≠æ"
    elif any(x in text for x in ["‰∏ã‰∏ã", "Âá∂", "Â§ßÂá∂"]):
        return "‰∏ã‰∏ãÁ≠æ"
    return "‰∏≠Á≠æ"

# ============================================================================
# GUAN YIN (ËßÇÈü≥ÁÅµÁ≠æ) Scraper
# ============================================================================

def scrape_guanyin_lot(lot_id: int) -> Optional[Dict]:
    """Scrape a single Guan Yin lot from various sources"""
    sources = [
        f"https://m.k366.com/qian/lqgy_{lot_id}.htm",  # Correct pattern for k366
        f"https://guanyin.hao86.com/qian/{lot_id}.html",
        f"https://m.zhouyi.cc/lingqian/guanyin/{lot_id}.html",
    ]
    
    for url in sources:
        soup = safe_request(url, retries=1)
        if soup is None:
            continue
        
        try:
            # Extract poem - Chinese divination poems are 4 lines of 7 characters each (28 chars total)
            poem_lines = []
            all_text = soup.get_text()
            
            # Method 1: Look for text in qian_table div after "Á≠æËØóÊñá" marker (MOST RELIABLE for k366.com)
            if not poem_lines or len(poem_lines) < 4:
                poem_elem = soup.select_one('.qian_table, [class*="qian"]')
                if poem_elem:
                    text = poem_elem.get_text()
                    # Extract Chinese characters from this div
                    chinese_chars = ''.join(re.findall(r'[\u4e00-\u9fff]', text))
                    
                    # Look for common poem starting words to find the right position
                    # But make sure we're not in the middle of other text
                    poem_starters = ['Ëè±Ëä±ÈïúÁ†¥', 'Â§©ÂºÄÂú∞Ëæü', 'È≤∏È±ºÊú™Âåñ', 'ÂÜ≤È£éÂÜíÈõ®']  # More specific patterns
                    for starter in poem_starters:
                        if starter in chinese_chars:
                            idx = chinese_chars.find(starter)
                            # Make sure we have enough characters after
                            if idx >= 0 and idx + 28 <= len(chinese_chars):
                                candidate = chinese_chars[idx:idx+28]
                                # Validate: should start with the starter
                                if candidate.startswith(starter[:2]):  # At least first 2 chars match
                                    # Split and validate
                                    lines = [candidate[j:j+7] for j in range(0, 28, 7)]
                                    if all(len(line) == 7 for line in lines):
                                        poem_lines = lines
                                        break
                    
                    # Alternative: look for the pattern after "Á≠æËØóÊñá" (most reliable for k366.com)
                    if not poem_lines or len(poem_lines) < 4:
                        if 'Á≠æËØóÊñá' in chinese_chars:
                            idx = chinese_chars.find('Á≠æËØóÊñá')
                            # Get 28 chars starting right after "Á≠æËØóÊñá" marker
                            start_idx = idx + len('Á≠æËØóÊñá')
                            if start_idx + 28 <= len(chinese_chars):
                                candidate = chinese_chars[start_idx:start_idx+28]
                                # Validate it's a proper poem (all 7-char lines)
                                lines = [candidate[j:j+7] for j in range(0, 28, 7)]
                                if all(len(line) == 7 for line in lines):
                                    # Additional check: should not contain markers
                                    if 'Á≠æ' not in candidate[7:] and 'ËØó' not in candidate[7:]:
                                        poem_lines = lines
                    
                    # If not found by starters, try the skip-words method
                    if not poem_lines or len(poem_lines) < 4:
                        skip_words = ['È¶ñÈ°µ', 'ÈªÑÂéÜ', 'ÊéíÁõò', 'ËøêÂäø', 'ÂÖ´Â≠ó', 'ËßÇÈü≥ÁÅµÁ≠æÁ¨¨', 'Á≠æËß£Á≠æ', 'ÂçéÊòìÁΩë', 'Á≠æËØóÊñá', 'ÂêâÂá∂', 'ÂÆ´‰Ωç', '‰∏≠‰∏≠Á≠æ', '‰∏≠‰∏äÁ≠æ', '‰∏ä‰∏äÁ≠æ']
                        for i in range(len(chinese_chars) - 27):
                            candidate = chinese_chars[i:i+28]
                            # Skip if contains navigation words
                            if any(word in candidate for word in skip_words):
                                continue
                            # Split and validate
                            lines = [candidate[j:j+7] for j in range(0, 28, 7)]
                            if all(len(line) == 7 for line in lines):
                                poem_lines = lines
                                break
            
            # Method 3: Look for continuous Chinese text in full page (fallback)
            if not poem_lines or len(poem_lines) < 4:
                chinese_only = re.sub(r'[^\u4e00-\u9fff]', '', all_text)
                # Find sequences of 28+ Chinese characters, but skip navigation/common words
                skip_words = ['È¶ñÈ°µ', 'ÈªÑÂéÜ', 'ÊéíÁõò', 'ËøêÂäø', 'ÂÖ´Â≠ó', 'ËßÇÈü≥ÁÅµÁ≠æÁ¨¨', 'Á≠æËß£Á≠æ', 'ÂçéÊòìÁΩë', 'Á≠æËØóÊñá']
                for i in range(len(chinese_only) - 27):
                    candidate = chinese_only[i:i+28]
                    # Skip if contains navigation words
                    if any(word in candidate for word in skip_words):
                        continue
                    # Split into 4 lines of 7 chars
                    lines = [candidate[j:j+7] for j in range(0, 28, 7)]
                    # Check if this looks like a poem (all lines are 7 chars, no repeated patterns)
                    if all(len(line) == 7 for line in lines) and len(set(lines)) >= 3:
                        # Additional validation: should not start with common prefixes
                        if not candidate.startswith(('ËßÇÈü≥', 'È¶ñÈ°µ', 'ÈªÑÂéÜ', 'ÊéíÁõò', 'Èü≥ÁÅµ')):
                            poem_lines = lines
                            break
            
            # Method 4: Look for 4 consecutive lines in the text
            if not poem_lines or len(poem_lines) < 4:
                lines = [l.strip() for l in all_text.split('\n') if l.strip()]
                for i in range(len(lines) - 3):
                    candidate = lines[i:i+4]
                    # Check if all are 7-char Chinese lines
                    if all(7 <= len(l) <= 8 and re.match(r'^[\u4e00-\u9fff]+$', l) for l in candidate):
                        poem_lines = candidate
                        break
            
            # Extract fortune from title or content
            fortune = "‰∏≠Á≠æ"
            title = soup.title.string if soup.title else ""
            if '‰∏ä‰∏ä' in title or 'Â§ßÂêâ' in title:
                fortune = "‰∏ä‰∏äÁ≠æ"
            elif '‰∏ä' in title and '‰∏ä‰∏ä' not in title:
                fortune = "‰∏äÁ≠æ"
            elif '‰∏≠‰∏≠' in title or '‰∏≠' in title:
                fortune = "‰∏≠Á≠æ"
            elif '‰∏≠‰∏ã' in title:
                fortune = "‰∏≠‰∏ãÁ≠æ"
            elif '‰∏ã‰∏ã' in title or 'Âá∂' in title:
                fortune = "‰∏ã‰∏ãÁ≠æ"
            
            # Also check in content
            content_text = soup.get_text()
            if '„ÄêÂêâÂá∂„Äë' in content_text or 'ÂêâÂá∂' in content_text:
                match = re.search(r'„ÄêÂêâÂá∂„Äë([^„Äê]+)', content_text)
                if not match:
                    match = re.search(r'ÂêâÂá∂[Ôºö:]([^\n]+)', content_text)
                if match:
                    fortune = normalize_fortune(match.group(1))
            
            # Extract story (usually in title or after poem)
            story = ""
            if '„Äê' in title:
                match = re.search(r'„Äê([^„Äë]+)„Äë', title)
                if match:
                    story = match.group(1)
            
            # Extract interpretation
            interpretation = ""
            if '„ÄêËØóÊÑè„Äë' in content_text or 'ËØóÊÑè' in content_text:
                match = re.search(r'„ÄêËØóÊÑè„Äë([^„Äê]+)', content_text)
                if not match:
                    match = re.search(r'ËØóÊÑè[Ôºö:]([^\n]+)', content_text)
                if match:
                    interpretation = match.group(1).strip()[:200]
            
            # Extract meanings from structured text
            meanings = []
            if 'ÂÆ∂ÂÆÖ' in content_text or 'Ëá™Ë∫´' in content_text:
                # Pattern: ÂÆ∂ÂÆÖ-Ê¨†Âà©ÔºåËá™Ë∫´-ÁßãÂÜ¨Êó∫
                meaning_pattern = r'([^Ôºå,]+?)[-Ôºç]([^Ôºå,]+?)(?=[Ôºå,]|$)'
                matches = re.findall(meaning_pattern, content_text)
                for label, value in matches[:15]:
                    label = label.strip()
                    value = value.strip()
                    if label and value and len(label) <= 10:
                        meanings.append({"label": label, "value": value})
            
            # Extract advice (usually at the end)
            advice = ""
            if '„ÄêËß£Êõ∞„Äë' in content_text or 'Ëß£Êõ∞' in content_text:
                match = re.search(r'„ÄêËß£Êõ∞„Äë([^„Äê]+)', content_text)
                if not match:
                    match = re.search(r'Ëß£Êõ∞[Ôºö:]([^\n]+)', content_text)
                if match:
                    advice = match.group(1).strip()[:300]
            
            # If we got the poem, return the lot
            if poem_lines and len(poem_lines) >= 4:
                return {
                    "id": lot_id,
                    "fortune": fortune,
                    "poem": poem_lines[:4],
                    "poemAnalysis": interpretation or f"Á¨¨{lot_id}Á≠æËØóÊÑèËß£Êûê",
                    "story": story or f"Á¨¨{lot_id}Á≠æÂÖ∏ÊïÖ",
                    "interpretation": interpretation or f"Á¨¨{lot_id}Á≠æËß£Êõ∞",
                    "meanings": meanings if meanings else [
                        {"label": "ÂÆ∂ÂÆÖ", "value": "ÂæÖË°•ÂÖÖ"},
                        {"label": "Ëá™Ë∫´", "value": "ÂæÖË°•ÂÖÖ"},
                        {"label": "Ê±ÇË¥¢", "value": "ÂæÖË°•ÂÖÖ"},
                        {"label": "‰∫§Êòì", "value": "ÂæÖË°•ÂÖÖ"},
                        {"label": "Â©öÂßª", "value": "ÂæÖË°•ÂÖÖ"},
                    ],
                    "advice": advice or f"Á¨¨{lot_id}Á≠æÂª∫ËÆÆ"
                }
        except Exception as e:
            continue
    
    return None

def collect_guanyin_lots() -> List[Dict]:
    """Collect ËßÇÈü≥ÁÅµÁ≠æ lots 4-100"""
    print("üìø Collecting ËßÇÈü≥ÁÅµÁ≠æ (Guan Yin)...")
    lots = []
    existing_ids = {1, 2, 3}  # Already have these
    
    # Try scraping all lots
    print("  üîç Attempting to scrape from online sources...")
    scraped_count = 0
    for i in range(4, 101):
        if i in existing_ids:
            continue
        
        # Check if we already have this lot
        if any(l.get('id') == i for l in lots):
            continue
        
        if i % 10 == 0:
            print(f"    Progress: {i}/100...", flush=True)
        
        lot = scrape_guanyin_lot(i)
        if lot and len(lot.get('poem', [])) == 4 and 'ÂæÖË°•ÂÖÖ' not in str(lot.get('poem', [])):
            lots.append(lot)
            scraped_count += 1
        else:
            # Create placeholder structure
            lot = {
            "id": i,
            "fortune": "‰∏≠Á≠æ",
            "poem": [f"Á≠æÊñáÁ¨¨{i}È¶ñÁ¨¨‰∏ÄÂè•", f"Á≠æÊñáÁ¨¨{i}È¶ñÁ¨¨‰∫åÂè•", f"Á≠æÊñáÁ¨¨{i}È¶ñÁ¨¨‰∏âÂè•", f"Á≠æÊñáÁ¨¨{i}È¶ñÁ¨¨ÂõõÂè•"],
            "poemAnalysis": f"Á¨¨{i}Á≠æËØóÊÑèËß£Êûê",
            "story": f"Á¨¨{i}Á≠æÂÖ∏ÊïÖ",
            "interpretation": f"Á¨¨{i}Á≠æËß£Êõ∞",
            "meanings": [
                {"label": "ÂÆ∂ÂÆÖ", "value": "ÂæÖË°•ÂÖÖ"},
                {"label": "Ëá™Ë∫´", "value": "ÂæÖË°•ÂÖÖ"},
                {"label": "Ê±ÇË¥¢", "value": "ÂæÖË°•ÂÖÖ"},
                {"label": "‰∫§Êòì", "value": "ÂæÖË°•ÂÖÖ"},
                {"label": "Â©öÂßª", "value": "ÂæÖË°•ÂÖÖ"},
            ],
            "advice": f"Á¨¨{i}Á≠æÂª∫ËÆÆ"
            }
            lots.append(lot)
        
        # Rate limiting - only delay if we got real data
        if lot and len(lot.get('poem', [])) == 4 and 'ÂæÖË°•ÂÖÖ' not in str(lot.get('poem', [])):
            time.sleep(0.3)  # Small delay for successful scrapes
        else:
            time.sleep(0.05)  # Very small delay for placeholders
    
    print(f"‚úÖ Collected {len(lots)} ËßÇÈü≥ÁÅµÁ≠æ lots ({scraped_count} scraped, {len(lots)-scraped_count} placeholders)")
    return lots

# ============================================================================
# WONG TAI SIN (ÈªÑÂ§ß‰ªô) Scraper
# ============================================================================

def scrape_wongtaisin_lot(lot_id: int) -> Optional[Dict]:
    """Scrape a single Wong Tai Sin lot - uses same structure as Guan Yin"""
    sources = [
        f"https://m.k366.com/qian/lqhdx_{lot_id}.htm",  # Primary source (same pattern as Guan Yin)
        f"https://m.zhouyi.cc/lingqian/huangdaxian/{lot_id}.html",
    ]
    
    for url in sources:
        soup = safe_request(url, retries=1)
        if soup is None:
            continue
        
        try:
            # Use same extraction logic as Guan Yin
            poem_lines = []
            all_text = soup.get_text()
            
            # Method 1: Look for text in qian_table div after "Á≠æËØó" marker
            poem_elem = soup.select_one('.qian_table, [class*="qian"]')
            if poem_elem:
                text = poem_elem.get_text()
                chinese_chars = ''.join(re.findall(r'[\u4e00-\u9fff]', text))
                
                # Look for pattern after "Á≠æËØó"
                if 'Á≠æËØó' in chinese_chars:
                    idx = chinese_chars.find('Á≠æËØó')
                    start_idx = idx + len('Á≠æËØó')
                    if start_idx + 28 <= len(chinese_chars):
                        candidate = chinese_chars[start_idx:start_idx+28]
                        lines = [candidate[j:j+7] for j in range(0, 28, 7)]
                        if all(len(line) == 7 for line in lines):
                            if 'Á≠æ' not in candidate[7:] and 'ËØó' not in candidate[7:]:
                                poem_lines = lines
            
            # Extract fortune, story, etc. (same as Guan Yin)
            fortune = "‰∏≠Á≠æ"
            title = soup.title.string if soup.title else ""
            if '‰∏ä‰∏ä' in title or 'Â§ßÂêâ' in title:
                fortune = "‰∏ä‰∏äÁ≠æ"
            elif '‰∏ä' in title and '‰∏ä‰∏ä' not in title:
                fortune = "‰∏äÁ≠æ"
            elif '‰∏≠‰∏≠' in title or '‰∏≠' in title:
                fortune = "‰∏≠Á≠æ"
            elif '‰∏≠‰∏ã' in title:
                fortune = "‰∏≠‰∏ãÁ≠æ"
            elif '‰∏ã‰∏ã' in title or 'Âá∂' in title:
                fortune = "‰∏ã‰∏ãÁ≠æ"
            
            # Extract story from title
            story = ""
            if '„Äê' in title or '„ÄÅ' in title:
                # Pattern: "Á¨¨XÁ≠æ_ÊïÖ‰∫ãÂêç„ÄÅ..."
                parts = re.split(r'[„ÄÅ_]', title)
                if len(parts) > 1:
                    story = parts[1].split('Ëß£Á≠æ')[0].strip()
            
            # Extract interpretation
            interpretation = ""
            content_text = soup.get_text()
            if '„ÄêËß£Êõ∞„Äë' in content_text or 'Ëß£Êõ∞' in content_text:
                match = re.search(r'„ÄêËß£Êõ∞„Äë([^„Äê]+)', content_text)
                if not match:
                    match = re.search(r'Ëß£Êõ∞[Ôºö:]([^\n]+)', content_text)
                if match:
                    interpretation = match.group(1).strip()[:200]
            
            # Extract advice
            advice = ""
            if '„ÄêËØ¶Ëß£„Äë' in content_text or 'ËØ¶Ëß£' in content_text:
                match = re.search(r'„ÄêËØ¶Ëß£„Äë([^„Äê]+)', content_text)
                if match:
                    advice = match.group(1).strip()[:300]
            
            if poem_lines and len(poem_lines) >= 4:
                return {
                    "id": lot_id,
                    "fortune": fortune,
                    "poem": poem_lines[:4],
                    "story": story or f"Á¨¨{lot_id}Á≠æÂÖ∏ÊïÖ",
                    "interpretation": interpretation or f"Á¨¨{lot_id}Á≠æËß£Á≠æ",
                    "advice": advice or f"Á¨¨{lot_id}Á≠æÂª∫ËÆÆ"
                }
        except Exception as e:
            continue
    
    return None

def collect_wongtaisin_lots() -> List[Dict]:
    """Collect ÈªÑÂ§ß‰ªôÁÅµÁ≠æ lots 2-100"""
    print("üèÆ Collecting ÈªÑÂ§ß‰ªôÁÅµÁ≠æ (Wong Tai Sin)...")
    lots = []
    existing_ids = {1}
    
    # Try scraping all lots
    print("  üîç Attempting to scrape from online sources...")
    scraped_count = 0
    for i in range(2, 101):
        if i in existing_ids:
            continue
        
        if any(l.get('id') == i for l in lots):
            continue
        
        if i % 10 == 0:
            print(f"    Progress: {i}/100...", flush=True)
        
        lot = scrape_wongtaisin_lot(i)
        if lot and len(lot.get('poem', [])) == 4 and 'ÂæÖË°•ÂÖÖ' not in str(lot.get('poem', [])):
            lots.append(lot)
            scraped_count += 1
        else:
            lot = {
                "id": i,
                "fortune": "‰∏≠Á≠æ",
                "poem": [f"ÈªÑÂ§ß‰ªôÁ¨¨{i}Á≠æÁ¨¨‰∏ÄÂè•", f"Á¨¨‰∫åÂè•", f"Á¨¨‰∏âÂè•", f"Á¨¨ÂõõÂè•"],
                "story": f"Á¨¨{i}Á≠æÂÖ∏ÊïÖ",
                "interpretation": f"Á¨¨{i}Á≠æËß£Á≠æ",
                "advice": f"Á¨¨{i}Á≠æÂª∫ËÆÆ"
            }
            lots.append(lot)
        
        if lot and len(lot.get('poem', [])) == 4 and 'ÂæÖË°•ÂÖÖ' not in str(lot.get('poem', [])):
            time.sleep(0.3)
        else:
            time.sleep(0.05)
    
    print(f"‚úÖ Collected {len(lots)} ÈªÑÂ§ß‰ªô lots ({scraped_count} scraped, {len(lots)-scraped_count} placeholders)")
    return lots

# ============================================================================
# YUE LAO (ÊúàËÄÅ) Scraper
# ============================================================================

def scrape_yuelao_lot(lot_id: int) -> Optional[Dict]:
    """Scrape a single Yue Lao lot - different format (not 7-char lines)"""
    sources = [
        f"https://m.smxs.com/ylyyq/jieqian/id/{lot_id}.html",  # Primary source
        f"https://services.shen88.cn/chouqian/yinyuan-{lot_id}.html",
    ]
    
    for url in sources:
        soup = safe_request(url, retries=1)
        if soup is None:
            continue
        
        try:
            poem_lines = []
            all_text = soup.get_text()
            
            # Extract poem from „ÄêÁ≠æÊñá„Äë marker
            # Yue Lao poems are typically 2 lines (14 chars total) or 4 shorter lines
            if '„ÄêÁ≠æÊñá„Äë' in all_text:
                parts = all_text.split('„ÄêÁ≠æÊñá„Äë')
                if len(parts) > 1:
                    poem_section = parts[1].split('„Äê')[0].strip()  # Get text before next marker
                    # Extract Chinese characters
                    chinese_chars = ''.join(re.findall(r'[\u4e00-\u9fff]', poem_section))
                    
                    # Yue Lao poems can be 2 lines of 7 chars (14 total) or 4 shorter lines
                    if len(chinese_chars) >= 14:
                        # Try 2 lines of 7 chars first
                        if len(chinese_chars) == 14:
                            poem_lines = [chinese_chars[0:7], chinese_chars[7:14]]
                        else:
                            # Split by punctuation for 4-line format
                            lines = re.split(r'[Ôºå„ÄÇÔºõ\n]', poem_section)
                            poem_lines = [l.strip() for l in lines if l.strip() and len(l.strip()) >= 3][:4]
                    else:
                        # Fallback: split by punctuation
                        lines = re.split(r'[Ôºå„ÄÇÔºõ\n]', poem_section)
                        poem_lines = [l.strip() for l in lines if l.strip() and len(l.strip()) >= 2][:4]
                    
                    # Ensure we have at least 2 lines, pad to 4 if needed
                    if len(poem_lines) == 2:
                        # Duplicate or pad to make 4 lines (some systems expect 4)
                        poem_lines = poem_lines + [''] * (4 - len(poem_lines))
                    elif len(poem_lines) < 2:
                        poem_lines = []
            
            # Extract fortune (Yue Lao uses different indicators)
            fortune = "‰∏≠Á≠æ"
            if '‰∏äÁ≠æ' in all_text or '‰∏ä‰∏äÁ≠æ' in all_text:
                fortune = "‰∏äÁ≠æ"
            elif '‰∏≠Á≠æ' in all_text:
                fortune = "‰∏≠Á≠æ"
            elif '‰∏ãÁ≠æ' in all_text or '‰∏ã‰∏ãÁ≠æ' in all_text:
                fortune = "‰∏ã‰∏ãÁ≠æ"
            
            # Extract story (usually in title or content)
            story = ""
            title = soup.title.string if soup.title else ""
            if 'Á¨¨' in title and 'Á≠æ' in title:
                # Extract story from title if available
                match = re.search(r'Á¨¨\d+Á≠æ[^Ëß£]*', title)
                if match:
                    story = match.group(0).replace('Á¨¨', '').replace('Á≠æ', '').strip()
            
            # Extract interpretation
            interpretation = ""
            if '„ÄêËß£Á≠æ„Äë' in all_text or 'Ëß£Á≠æ' in all_text:
                match = re.search(r'„ÄêËß£Á≠æ„Äë([^„Äê]+)', all_text)
                if not match:
                    match = re.search(r'Ëß£Á≠æ[Ôºö:]([^\n]+)', all_text)
                if match:
                    interpretation = match.group(1).strip()[:200]
            
            # Extract advice (from various sections)
            advice = ""
            # Yue Lao has specific indices - extract if available
            if '„ÄêÁºò‰ªΩÊåáÊï∞„Äë' in all_text or '„ÄêÂπ∏Á¶èÊåáÊï∞„Äë' in all_text:
                # Extract index information as advice
                indices = []
                for idx_type in ['Áºò‰ªΩÊåáÊï∞', 'Âπ∏Á¶èÊåáÊï∞', 'ÊößÊòßÊåáÊï∞', 'Áº†ÁªµÊåáÊï∞']:
                    match = re.search(rf'„Äê{idx_type}„Äë[^\d]*(\d+)', all_text)
                    if match:
                        indices.append(f'{idx_type}: {match.group(1)}')
                if indices:
                    advice = 'Ôºõ'.join(indices)
            
            # Yue Lao poems can be 2 or 4 lines
            if poem_lines and len(poem_lines) >= 2:
                # Ensure we have 4 elements (pad with empty strings if needed)
                while len(poem_lines) < 4:
                    poem_lines.append('')
                return {
                    "id": lot_id,
                    "fortune": fortune,
                    "poem": poem_lines[:4],
                    "story": story or f"Á¨¨{lot_id}Á≠æÂÖ∏ÊïÖ",
                    "interpretation": interpretation or f"Á¨¨{lot_id}Á≠æËß£Á≠æ",
                    "advice": advice or f"Á¨¨{lot_id}Á≠æÂßªÁºòÂª∫ËÆÆ"
                }
        except Exception as e:
            continue
    
    return None

def collect_yuelao_lots() -> List[Dict]:
    """Collect ÊúàËÄÅÁÅµÁ≠æ lots 2-60"""
    print("üíï Collecting ÊúàËÄÅÁÅµÁ≠æ (Yue Lao)...")
    lots = []
    existing_ids = {1}
    
    # Try scraping all lots
    print("  üîç Attempting to scrape from online sources...")
    scraped_count = 0
    for i in range(2, 61):
        if i in existing_ids:
            continue
        
        if any(l.get('id') == i for l in lots):
            continue
        
        if i % 10 == 0:
            print(f"    Progress: {i}/60...", flush=True)
        
        lot = scrape_yuelao_lot(i)
        if lot and len(lot.get('poem', [])) >= 2 and any(l.strip() for l in lot.get('poem', [])[:2]) and 'ÂæÖË°•ÂÖÖ' not in str(lot.get('poem', [])):
            lots.append(lot)
            scraped_count += 1
        else:
            lot = {
                "id": i,
                "fortune": "‰∏≠Á≠æ",
                "poem": [f"ÊúàËÄÅÁ¨¨{i}Á≠æÁ¨¨‰∏ÄÂè•", f"Á¨¨‰∫åÂè•", f"Á¨¨‰∏âÂè•", f"Á¨¨ÂõõÂè•"],
                "story": f"Á¨¨{i}Á≠æÂÖ∏ÊïÖ",
                "interpretation": f"Á¨¨{i}Á≠æËß£Á≠æ",
                "advice": f"Á¨¨{i}Á≠æÂßªÁºòÂª∫ËÆÆ"
            }
            lots.append(lot)
        
        if lot and len(lot.get('poem', [])) == 4 and 'ÂæÖË°•ÂÖÖ' not in str(lot.get('poem', [])):
            time.sleep(0.3)
        else:
            time.sleep(0.05)
    
    print(f"‚úÖ Collected {len(lots)} ÊúàËÄÅ lots ({scraped_count} scraped, {len(lots)-scraped_count} placeholders)")
    return lots

# ============================================================================
# LV ZU (ÂêïÁ•ñ) Scraper
# ============================================================================

def scrape_lvzu_lot(lot_id: int) -> Optional[Dict]:
    """Scrape a single Lu Zu lot - uses same structure as Guan Yin"""
    sources = [
        f"https://m.k366.com/qian/lqlz_{lot_id}.htm",  # Primary source (same pattern)
        f"https://m.zhouyi.cc/lingqian/lvzu/{lot_id}.html",
    ]
    
    for url in sources:
        soup = safe_request(url, retries=1)
        if soup is None:
            continue
        
        try:
            # Use same extraction logic as Guan Yin
            poem_lines = []
            all_text = soup.get_text()
            
            # Method 1: Look for text in qian_table div after "Á≠æËØó" marker
            poem_elem = soup.select_one('.qian_table, [class*="qian"]')
            if poem_elem:
                text = poem_elem.get_text()
                chinese_chars = ''.join(re.findall(r'[\u4e00-\u9fff]', text))
                
                # Look for pattern after "Á≠æËØó"
                if 'Á≠æËØó' in chinese_chars:
                    idx = chinese_chars.find('Á≠æËØó')
                    start_idx = idx + len('Á≠æËØó')
                    if start_idx + 28 <= len(chinese_chars):
                        candidate = chinese_chars[start_idx:start_idx+28]
                        lines = [candidate[j:j+7] for j in range(0, 28, 7)]
                        if all(len(line) == 7 for line in lines):
                            if 'Á≠æ' not in candidate[7:] and 'ËØó' not in candidate[7:]:
                                poem_lines = lines
            
            # Extract fortune, story, etc.
            fortune = "‰∏≠Á≠æ"
            title = soup.title.string if soup.title else ""
            if '‰∏ä‰∏ä' in title or 'Â§ßÂêâ' in title:
                fortune = "‰∏ä‰∏äÁ≠æ"
            elif '‰∏ä' in title and '‰∏ä‰∏ä' not in title:
                fortune = "‰∏äÁ≠æ"
            elif '‰∏≠‰∏≠' in title or '‰∏≠' in title:
                fortune = "‰∏≠Á≠æ"
            elif '‰∏≠‰∏ã' in title:
                fortune = "‰∏≠‰∏ãÁ≠æ"
            elif '‰∏ã‰∏ã' in title or 'Âá∂' in title:
                fortune = "‰∏ã‰∏ãÁ≠æ"
            
            # Extract story from title
            story = ""
            if '„Äê' in title or '„ÄÅ' in title or 'Âè§‰∫∫' in title:
                # Pattern: "Á¨¨XÁ≠æ_Âè§‰∫∫ÊïÖ‰∫ãÂêç„ÄÅ..."
                parts = re.split(r'[„ÄÅ_]', title)
                if len(parts) > 1:
                    story = parts[1].split('Ëß£Á≠æ')[0].replace('Âè§‰∫∫', '').strip()
            
            # Extract interpretation
            interpretation = ""
            content_text = soup.get_text()
            if '„ÄêËß£Êõ∞„Äë' in content_text or 'Ëß£Êõ∞' in content_text:
                match = re.search(r'„ÄêËß£Êõ∞„Äë([^„Äê]+)', content_text)
                if not match:
                    match = re.search(r'Ëß£Êõ∞[Ôºö:]([^\n]+)', content_text)
                if match:
                    interpretation = match.group(1).strip()[:200]
            
            # Extract advice
            advice = ""
            if '„ÄêËØ¶Ëß£„Äë' in content_text or 'ËØ¶Ëß£' in content_text:
                match = re.search(r'„ÄêËØ¶Ëß£„Äë([^„Äê]+)', content_text)
                if match:
                    advice = match.group(1).strip()[:300]
            
            if poem_lines and len(poem_lines) >= 4:
                return {
                    "id": lot_id,
                    "fortune": fortune,
                    "poem": poem_lines[:4],
                    "story": story or f"Á¨¨{lot_id}Á≠æÂÖ∏ÊïÖ",
                    "interpretation": interpretation or f"Á¨¨{lot_id}Á≠æËß£Á≠æ",
                    "advice": advice or f"Á¨¨{lot_id}Á≠æ‰øÆË°åÂª∫ËÆÆ"
                }
        except Exception as e:
            continue
    
    return None

def collect_lvzu_lots() -> List[Dict]:
    """Collect ÂêïÁ•ñÁÅµÁ≠æ lots 2-100"""
    print("‚ö° Collecting ÂêïÁ•ñÁÅµÁ≠æ (Lu Zu)...")
    lots = []
    existing_ids = {1}
    
    # Try scraping all lots
    print("  üîç Attempting to scrape from online sources...")
    scraped_count = 0
    for i in range(2, 101):
        if i in existing_ids:
            continue
        
        if any(l.get('id') == i for l in lots):
            continue
        
        if i % 10 == 0:
            print(f"    Progress: {i}/100...", flush=True)
        
        lot = scrape_lvzu_lot(i)
        if lot and len(lot.get('poem', [])) == 4 and 'ÂæÖË°•ÂÖÖ' not in str(lot.get('poem', [])):
            lots.append(lot)
            scraped_count += 1
        else:
            lot = {
                "id": i,
                "fortune": "‰∏≠Á≠æ",
                "poem": [f"ÂêïÁ•ñÁ¨¨{i}Á≠æÁ¨¨‰∏ÄÂè•", f"Á¨¨‰∫åÂè•", f"Á¨¨‰∏âÂè•", f"Á¨¨ÂõõÂè•"],
                "story": f"Á¨¨{i}Á≠æÂÖ∏ÊïÖ",
                "interpretation": f"Á¨¨{i}Á≠æËß£Á≠æ",
                "advice": f"Á¨¨{i}Á≠æ‰øÆË°åÂª∫ËÆÆ"
            }
            lots.append(lot)
        
        if lot and len(lot.get('poem', [])) == 4 and 'ÂæÖË°•ÂÖÖ' not in str(lot.get('poem', [])):
            time.sleep(0.3)
        else:
            time.sleep(0.05)
    
    print(f"‚úÖ Collected {len(lots)} ÂêïÁ•ñ lots ({scraped_count} scraped, {len(lots)-scraped_count} placeholders)")
    return lots

# ============================================================================
# MAZU (Â¶àÁ•ñ) Scraper
# ============================================================================

def scrape_mazu_lot(lot_id: int) -> Optional[Dict]:
    """Scrape a single Mazu lot - different format"""
    sources = [
        f"https://m.smxs.com/mazu/jieqian/id/{lot_id}.html",  # Primary source
        f"https://services.shen88.cn/chouqian/tianhou-{lot_id}.html",
    ]
    
    for url in sources:
        soup = safe_request(url, retries=1)
        if soup is None:
            continue
        
        try:
            poem_lines = []
            all_text = soup.get_text()
            
            # Extract poem from „ÄêÁ≠æËØó„Äë marker
            if '„ÄêÁ≠æËØó„Äë' in all_text:
                parts = all_text.split('„ÄêÁ≠æËØó„Äë')
                if len(parts) > 1:
                    poem_section = parts[1].split('„Äê')[0].strip()  # Get text before next marker
                    # Split by punctuation (Ôºå„ÄÇÔºõ)
                    lines = re.split(r'[Ôºå„ÄÇÔºõ\n]', poem_section)
                    poem_lines = [l.strip() for l in lines if l.strip() and len(l.strip()) >= 3][:4]
            
            # Extract fortune from title
            fortune = "‰∏≠Á≠æ"
            title = soup.title.string if soup.title else ""
            if '‰∏äÁ≠æ' in title or '‰∏ä‰∏äÁ≠æ' in title:
                fortune = "‰∏äÁ≠æ"
            elif '‰∏≠Á≠æ' in title:
                fortune = "‰∏≠Á≠æ"
            elif '‰∏ãÁ≠æ' in title or '‰∏ã‰∏ãÁ≠æ' in title:
                fortune = "‰∏ã‰∏ãÁ≠æ"
            
            # Extract story from „ÄêÁ≠æËØóÂÖ∏ÊïÖ„Äë
            story = ""
            if '„ÄêÁ≠æËØóÂÖ∏ÊïÖ„Äë' in all_text or 'ÂÖ∏ÊïÖ' in all_text:
                match = re.search(r'„ÄêÁ≠æËØóÂÖ∏ÊïÖ„Äë([^„Äê]+)', all_text)
                if not match:
                    # Look for story name in title
                    if '„Äê' in title:
                        match = re.search(r'„Äê([^„Äë]+)„Äë', title)
                        if match:
                            story = match.group(1).split('„ÄÅ')[0].strip()
            
            # Extract interpretation from „ÄêËß£Êõ∞„Äë
            interpretation = ""
            if '„ÄêËß£Êõ∞„Äë' in all_text:
                match = re.search(r'„ÄêËß£Êõ∞„Äë([^„Äê]+)', all_text)
                if match:
                    interpretation = match.group(1).strip()[:200]
            
            # Extract advice from „ÄêÁ≠æËØóËØ≠ËØë„Äë or other sections
            advice = ""
            if '„ÄêÁ≠æËØóËØ≠ËØë„Äë' in all_text:
                match = re.search(r'„ÄêÁ≠æËØóËØ≠ËØë„Äë([^„Äê]+)', all_text)
                if match:
                    advice = match.group(1).strip()[:300]
            
            if poem_lines and len(poem_lines) >= 4:
                return {
                    "id": lot_id,
                    "fortune": fortune,
                    "poem": poem_lines[:4],
                    "story": story or f"Á¨¨{lot_id}Á≠æÂÖ∏ÊïÖ",
                    "interpretation": interpretation or f"Á¨¨{lot_id}Á≠æËß£Á≠æ",
                    "advice": advice or f"Á¨¨{lot_id}Á≠æÂá∫Ë°åÂª∫ËÆÆ"
                }
        except Exception as e:
            continue
    
    return None

def collect_mazu_lots() -> List[Dict]:
    """Collect Â¶àÁ•ñÁÅµÁ≠æ lots 2-60"""
    print("üåä Collecting Â¶àÁ•ñÁÅµÁ≠æ (Mazu)...")
    lots = []
    existing_ids = {1}
    
    # Try scraping all lots
    print("  üîç Attempting to scrape from online sources...")
    scraped_count = 0
    for i in range(2, 61):
        if i in existing_ids:
            continue
        
        if any(l.get('id') == i for l in lots):
            continue
        
        if i % 10 == 0:
            print(f"    Progress: {i}/60...", flush=True)
        
        lot = scrape_mazu_lot(i)
        if lot and len(lot.get('poem', [])) == 4 and 'ÂæÖË°•ÂÖÖ' not in str(lot.get('poem', [])):
            lots.append(lot)
            scraped_count += 1
        else:
            lot = {
                "id": i,
                "fortune": "‰∏≠Á≠æ",
                "poem": [f"Â¶àÁ•ñÁ¨¨{i}Á≠æÁ¨¨‰∏ÄÂè•", f"Á¨¨‰∫åÂè•", f"Á¨¨‰∏âÂè•", f"Á¨¨ÂõõÂè•"],
                "story": f"Á¨¨{i}Á≠æÂÖ∏ÊïÖ",
                "interpretation": f"Á¨¨{i}Á≠æËß£Á≠æ",
                "advice": f"Á¨¨{i}Á≠æÂá∫Ë°åÂª∫ËÆÆ"
            }
            lots.append(lot)
        
        if lot and len(lot.get('poem', [])) == 4 and 'ÂæÖË°•ÂÖÖ' not in str(lot.get('poem', [])):
            time.sleep(0.3)
        else:
            time.sleep(0.05)
    
    print(f"‚úÖ Collected {len(lots)} Â¶àÁ•ñ lots ({scraped_count} scraped, {len(lots)-scraped_count} placeholders)")
    return lots

# ============================================================================
# Data Export Functions
# ============================================================================

def save_to_json(all_data: Dict, output_file: str = "collected_divination_data.json"):
    """Save collected data to JSON file"""
    print(f"\nüíæ Saving to JSON: {output_file}...")
    
    output_path = Path(output_file)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(all_data, f, ensure_ascii=False, indent=2)
    
    print(f"‚úÖ Saved to {output_path.absolute()}")

def escape_ts_string(s: str) -> str:
    """Escape string for TypeScript"""
    if s is None:
        return ""
    return s.replace('\\', '\\\\').replace('"', '\\"').replace('\n', '\\n').replace('\r', '')

def format_ts_value(value) -> str:
    """Format a Python value as TypeScript"""
    if isinstance(value, str):
        return f'"{escape_ts_string(value)}"'
    elif isinstance(value, (list, tuple)):
        items = ', '.join(format_ts_value(item) for item in value)
        return f'[{items}]'
    elif isinstance(value, dict):
        items = ', '.join(f'"{k}": {format_ts_value(v)}' for k, v in value.items())
        return f'{{{items}}}'
    elif value is None:
        return '""'
    else:
        return str(value)

def generate_typescript_file(all_data: Dict, output_file: str = "src/data/collected_divination_data.ts"):
    """Generate TypeScript file with collected data"""
    print(f"\nüíæ Generating TypeScript file: {output_file}...")
    
    output_path = Path(output_file)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    ts_content = '''// Collected Divination Data
// Auto-generated by collect_all_divination_data.py
// This file contains all collected lots from 5 Chinese divination systems

export interface DivinationSystemLot {
  id: number;
  fortune: string;
  poem: string[];
  story: string;
  interpretation: string;
  advice: string;
}

export interface GuanYinLot extends DivinationSystemLot {
  poemAnalysis: string;
  meanings: { label: string; value: string }[];
}

export const COLLECTED_DIVINATION_DATA = {
'''
    
    # Add each system's data
    for system_name, lots in all_data.items():
        ts_content += f'  {system_name}: [\n'
        for lot in lots:
            ts_content += '    {\n'
            ts_content += f'      id: {lot["id"]},\n'
            ts_content += f'      fortune: {format_ts_value(lot["fortune"])},\n'
            ts_content += f'      poem: {format_ts_value(lot["poem"])},\n'
            
            if "poemAnalysis" in lot:
                ts_content += f'      poemAnalysis: {format_ts_value(lot["poemAnalysis"])},\n'
            
            ts_content += f'      story: {format_ts_value(lot["story"])},\n'
            ts_content += f'      interpretation: {format_ts_value(lot["interpretation"])},\n'
            
            if "meanings" in lot:
                ts_content += '      meanings: [\n'
                for meaning in lot["meanings"]:
                    ts_content += f'        {{ label: {format_ts_value(meaning["label"])}, value: {format_ts_value(meaning["value"])} }},\n'
                ts_content += '      ],\n'
            
            ts_content += f'      advice: {format_ts_value(lot["advice"])},\n'
            ts_content += '    },\n'
        
        ts_content += '  ],\n'
    
    ts_content += '''} as const;

export default COLLECTED_DIVINATION_DATA;
'''
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(ts_content)
    
    print(f"‚úÖ Generated TypeScript file: {output_path.absolute()}")

def main():
    print("üöÄ Starting Divination Data Collection", flush=True)
    print("=" * 60, flush=True)
    print("This will collect 413 lots from 5 Chinese divination systems", flush=True)
    print("Estimated time: 10-15 minutes (with rate limiting)", flush=True)
    print("=" * 60, flush=True)
    print(flush=True)
    
    start_time = time.time()
    sys.stdout.flush()
    
    all_data = {
        "guanyin": collect_guanyin_lots(),
        "wongtaisin": collect_wongtaisin_lots(),
        "yuelao": collect_yuelao_lots(),
        "lvzu": collect_lvzu_lots(),
        "mazu": collect_mazu_lots()
    }
    
    total_collected = sum(len(lots) for lots in all_data.values())
    elapsed_time = time.time() - start_time
    
    print(f"\nüéâ Collection Complete!")
    print(f"üìä Total lots collected: {total_collected}/413")
    print(f"‚è±Ô∏è  Time elapsed: {elapsed_time/60:.1f} minutes")
    
    # Save to JSON
    save_to_json(all_data)
    
    # Generate TypeScript file
    generate_typescript_file(all_data)
    
    print("\n‚úÖ All done!")
    print("\nüìù Next steps:")
    print("   1. Review collected_divination_data.json")
    print("   2. Check src/data/collected_divination_data.ts")
    print("   3. Merge with existing data in chineseDivinationSystems.ts")
    print("\n‚ö†Ô∏è  NOTE: Some lots may have placeholder data if scraping failed.")
    print("   You may need to manually verify and update specific lots.")

if __name__ == "__main__":
    main()
